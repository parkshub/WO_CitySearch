{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52f425db",
   "metadata": {},
   "source": [
    "# CitySearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d3656f",
   "metadata": {},
   "source": [
    "## Webscraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836ad022",
   "metadata": {},
   "source": [
    "#### **Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce89f2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from requests_html import AsyncHTMLSession\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, \\\n",
    "    NoSuchElementException, WebDriverException, StaleElementReferenceException"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42baf65c",
   "metadata": {},
   "source": [
    "#### **Global Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4377442",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "us_state_to_abbrev = {\n",
    "    \"Alabama\": \"AL\",\n",
    "    \"Alaska\": \"AK\",\n",
    "    \"Arizona\": \"AZ\",\n",
    "    \"Arkansas\": \"AR\",\n",
    "    \"California\": \"CA\",\n",
    "    \"Colorado\": \"CO\",\n",
    "    \"Connecticut\": \"CT\",\n",
    "    \"Delaware\": \"DE\",\n",
    "    \"Florida\": \"FL\",\n",
    "    \"Georgia\": \"GA\",\n",
    "    \"Hawaii\": \"HI\",\n",
    "    \"Idaho\": \"ID\",\n",
    "    \"Illinois\": \"IL\",\n",
    "    \"Indiana\": \"IN\",\n",
    "    \"Iowa\": \"IA\",\n",
    "    \"Kansas\": \"KS\",\n",
    "    \"Kentucky\": \"KY\",\n",
    "    \"Louisiana\": \"LA\",\n",
    "    \"Maine\": \"ME\",\n",
    "    \"Maryland\": \"MD\",\n",
    "    \"Massachusetts\": \"MA\",\n",
    "    \"Michigan\": \"MI\",\n",
    "    \"Minnesota\": \"MN\",\n",
    "    \"Mississippi\": \"MS\",\n",
    "    \"Missouri\": \"MO\",\n",
    "    \"Montana\": \"MT\",\n",
    "    \"Nebraska\": \"NE\",\n",
    "    \"Nevada\": \"NV\",\n",
    "    \"New Hampshire\": \"NH\",\n",
    "    \"New Jersey\": \"NJ\",\n",
    "    \"New Mexico\": \"NM\",\n",
    "    \"New York\": \"NY\",\n",
    "    \"North Carolina\": \"NC\",\n",
    "    \"North Dakota\": \"ND\",\n",
    "    \"Ohio\": \"OH\",\n",
    "    \"Oklahoma\": \"OK\",\n",
    "    \"Oregon\": \"OR\",\n",
    "    \"Pennsylvania\": \"PA\",\n",
    "    \"Rhode Island\": \"RI\",\n",
    "    \"South Carolina\": \"SC\",\n",
    "    \"South Dakota\": \"SD\",\n",
    "    \"Tennessee\": \"TN\",\n",
    "    \"Texas\": \"TX\",\n",
    "    \"Utah\": \"UT\",\n",
    "    \"Vermont\": \"VT\",\n",
    "    \"Virginia\": \"VA\",\n",
    "    \"Washington\": \"WA\",\n",
    "    \"West Virginia\": \"WV\",\n",
    "    \"Wisconsin\": \"WI\",\n",
    "    \"Wyoming\": \"WY\",\n",
    "    \"District of Columbia\": \"DC\",\n",
    "    \"American Samoa\": \"AS\",\n",
    "    \"Guam\": \"GU\",\n",
    "    \"Northern Mariana Islands\": \"MP\",\n",
    "    \"Puerto Rico\": \"PR\",\n",
    "    \"United States Minor Outlying Islands\": \"UM\",\n",
    "    \"U.S. Virgin Islands\": \"VI\",\n",
    "}\n",
    "\n",
    "industries = [\n",
    "    {\n",
    "        \"Construction\": [\n",
    "            \"Carpentry\", \n",
    "            \"Plumbing\", \n",
    "            \"Electrical work\"\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"Manufacturing\": [\n",
    "            \"Welding\", \n",
    "            \"Machine operation\", \n",
    "            \"Assembly line work\"\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"Transportation\": [\n",
    "            \"Truck driving\", \n",
    "            \"Warehouse operations\", \n",
    "            \"Forklift operation\"\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"Logistics\": []\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"Automotive\": [\n",
    "            \"Automotive repair\", \n",
    "            \"Auto maintenance\", \n",
    "            \"Auto Bodywork\", \n",
    "            \"Tire services\"\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"Maintenance and Repair\": [\n",
    "            \"HVAC\", \n",
    "            \"Appliance repair\", \n",
    "            \"General maintenance\"\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"Retail\": [\n",
    "            \"Boutiques\", \n",
    "            \"Specialty stores\", \n",
    "            \"Online shops\"\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"Food and Beverage\": [\n",
    "            \"Restaurants\", \n",
    "            \"Cafes\", \n",
    "            \"Food trucks\"\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"Personal Services\": [\n",
    "            \"Hair salons\", \n",
    "            \"Barber shops\"\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "states_of_interest = [\"California\", \"New Jersey\", \"New York\", \"Texas\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daea086",
   "metadata": {},
   "source": [
    "#### **Functions Used**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba5cdd1",
   "metadata": {},
   "source": [
    "#### There's a bit going on, so I'll try my best to explain what each function does in the order they are called. I hope it helps understand the main implementation better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a711c1",
   "metadata": {},
   "source": [
    "##### ***switch()***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d159a1",
   "metadata": {},
   "source": [
    "The switch funciton is simply a quality-of-life function to switch the orders of an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1c4c7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def switch(el):\n",
    "    pos1, pos2 = el.split(\"/\")\n",
    "    return pos2 + \",%20\" + pos1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85c2d4b",
   "metadata": {},
   "source": [
    "##### ***get_job_cards_links()***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5bf809",
   "metadata": {},
   "source": [
    "In CitySearch, per industry and location, there's a list of companies. Here we're locating and saving the links to each company profile. Handling exceptions at this stage wasn't too much of an issue, but on rare occasions there were no companies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5965c4a",
   "metadata": {},
   "source": [
    "<img src=\"assets/job_list.png\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d61344a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_job_cards_links(driver):\n",
    "    try:\n",
    "        print('looking for job card list')\n",
    "\n",
    "        job_cards = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located(\n",
    "                (By.CSS_SELECTOR, \"div.list-container > div.card > a\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "        job_cards_links = [job.get_attribute(\"href\") for job in job_cards]\n",
    "\n",
    "    except (NoSuchElementException, TimeoutException):\n",
    "        print(\"Error: Timed out waiting for page to load. \\\n",
    "              Most likely no job listing in this category\")\n",
    "\n",
    "        job_cards_links = []\n",
    "\n",
    "    return job_cards_links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb3f62c",
   "metadata": {},
   "source": [
    "##### ***business_details_to_dict()***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bca597",
   "metadata": {},
   "source": [
    "business_details_to_dict receives a link from the list created above and scrapes business details, such as name, address, phone number, link to webiste, business hours, and additional details. However, not all businesses had a link to their website. Lastly, I decided to include additional details to get a better idea of what the company actually does. Although the information isn't always useful, it provides important insight especially when it isn't clear whether a company belongs in a specific industry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52595256",
   "metadata": {},
   "source": [
    "<img src=\"assets/business_details.png\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb63bfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def business_details_to_dict(driver, industry):\n",
    "    business_details_dict = {'industry': industry}\n",
    "\n",
    "    try:\n",
    "        print('looking for business details')\n",
    "        business_details = WebDriverWait(driver, 5).until(\n",
    "            EC.presence_of_all_elements_located(\n",
    "                (By.CSS_SELECTOR, \"div.business-details > *\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "    except (NoSuchElementException, TimeoutException, StaleElementReferenceException):\n",
    "        print(\"Timed out waiting for business details to load\")\n",
    "        return {\n",
    "                    \"business-name\": \"\", \n",
    "                    \"address\": \"\", \n",
    "                    \"external-links-container\": \"\", \n",
    "                    \"phone-trigger\": \"\", \n",
    "                    \"industry\": \"\",\n",
    "                    \"additional_info\": \"\", \n",
    "                    \"emails\": []\n",
    "                }\n",
    "\n",
    "    for entry in business_details:\n",
    "        try:\n",
    "            class_name = entry.get_attribute(\"class\")\n",
    "\n",
    "            if class_name == \"external-links-container\":\n",
    "                try:\n",
    "                    print('looking for external links')\n",
    "\n",
    "                    elem = WebDriverWait(driver, 3).until(\n",
    "                        EC.presence_of_element_located(\n",
    "                            (\n",
    "                                By.CSS_SELECTOR, \n",
    "                                \"div.external-links-container a\"\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    business_details_dict[class_name] = elem.get_attribute(\"href\")\n",
    "\n",
    "                except (NoSuchElementException, TimeoutException):\n",
    "                    print(\"Timed out waiting for external link: \\\n",
    "                          No link to website\")\n",
    "                    business_details_dict[class_name] = \"\"\n",
    "\n",
    "            else:\n",
    "                business_details_dict[class_name] = entry.text\n",
    "\n",
    "        except StaleElementReferenceException:\n",
    "            print(\"Error: Stale reference exception\")\n",
    "\n",
    "    return business_details_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b24200",
   "metadata": {},
   "source": [
    "##### ***get_email()*** ***&*** ***html_to_string()***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e74b16",
   "metadata": {},
   "source": [
    "Below, I've grouped get_email and html_to_string together because they're always called togehter. \n",
    "\n",
    "If business_details_to_dict returns a dictionary that includes a company's website, get_email is called and inside get_email, html_to_string is called.\n",
    "\n",
    "get_email receives the link and send the link to html_to_string. html_to_string parses the page, replaces all whitespace (i.e. \\n, \\t, etc.) with a space and returns a stringified version of the page. get_email then uses regex to detect all emails within the current page.\n",
    "\n",
    "The process of replacing whitespace and returning a string was mainly done to avoid emails being scraped incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e571accb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "async def get_email(url, email_set):\n",
    "    print(\"getting emails\")\n",
    "\n",
    "    pattern = \"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n",
    "    body = await html_to_string(url)\n",
    "\n",
    "    if body is None:\n",
    "        return\n",
    "\n",
    "    emails = re.findall(pattern, body)\n",
    "    email_set.update(emails)\n",
    "\n",
    "async def html_to_string(url):\n",
    "    session = AsyncHTMLSession()\n",
    "\n",
    "    try:\n",
    "        r = await session.get(url, verify=False)\n",
    "        soup = BeautifulSoup(r.html.raw_html, \"html.parser\")\n",
    "        body = soup.find('body')\n",
    "        return body.get_text(separator=\" \").strip()\n",
    "\n",
    "    except:\n",
    "        print(\"Error: Website does not exist\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7444a00b",
   "metadata": {},
   "source": [
    "##### ***get_email_from_contact()***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75687993",
   "metadata": {},
   "source": [
    "get_email_from_contact also uses the external link provided by business_details_to_dict. From a companies main page, it looks for the existence of a contact page (i.e. /contact, /Contact, /CONTACT, /contact-us, /Contact-Us, /CONTACT-US). If a contact page is found, it calls get_email and follows the same process as above to extract email addresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d397bd05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "async def get_email_from_contact(driver, url, email_set):\n",
    "    try:\n",
    "        print('looking for email from contact')\n",
    "        driver.get(url)\n",
    "\n",
    "        contact_button = WebDriverWait(driver, 5).until(\n",
    "            EC.presence_of_element_located(\n",
    "                (\n",
    "                    By.CSS_SELECTOR, \n",
    "                    \"a[href*='contact'], \\\n",
    "                    a[href*='Contact'], \\\n",
    "                    a[href*='CONTACT']\"\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        driver.get(contact_button.get_attribute(\"href\"))\n",
    "        url_to_scrape = driver.current_url\n",
    "\n",
    "        await get_email(url_to_scrape, email_set)\n",
    "\n",
    "    except (NoSuchElementException, TimeoutException, WebDriverException):\n",
    "        print(\"Error: No contact page\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fd76e8",
   "metadata": {},
   "source": [
    "##### ***save_to_csv()***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b679f48",
   "metadata": {},
   "source": [
    "This is the last step of each loop. It simply exports all the information gathered into a .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076267cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def save_to_csv(business_list, where_param):\n",
    "    df = pd.DataFrame.from_dict(business_list)\n",
    "\n",
    "    df.rename(columns={\n",
    "        \"business-name\": \"business name\",\n",
    "        \"external-links-container\": \"external link\",\n",
    "        \"phone-trigger\": \"phone number\",\n",
    "        \"business-hours\": \"business hours\"\n",
    "    }, inplace=True)\n",
    "\n",
    "    df.to_csv(f'./{where_param.replace(\",%20\", \"_\").replace(\"%20\", \"_\")}.csv', \n",
    "              index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d2fae0",
   "metadata": {},
   "source": [
    "### **Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ba91da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###########################################################################################################\n",
    "#------iterating over all the states, then cities, then industries and scarping business information------#\n",
    "async def main():\n",
    "    ############################################\n",
    "    #------opening and cleaning xlsx file------#\n",
    "    df = pd.read_excel(\"assets/google_maps_keywords.xlsx\")\n",
    "    df.loc[:, [\"Country\", \"State\"]] = df.loc[:, [\"Country\", \"State\"]].ffill()\n",
    "\n",
    "    #########################################################\n",
    "    #------grouping dataframe by country then by state------#\n",
    "    grouped_df = df.groupby(\"Country\")\n",
    "    grouped_countries = grouped_df.get_group(\"United States\")\n",
    "    grouped_states = grouped_countries.groupby(\"State\")\n",
    "    states = grouped_states.groups.keys()\n",
    "\n",
    "    ########################################################\n",
    "    #------navigating to the front page of CitySearch------#\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\"--proxy-server='direct://'\")\n",
    "    chrome_options.add_argument(\"--proxy-bypass-list=*\")\n",
    "    chrome_options.add_argument(\"--start-maximized\")\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--disable-gpu')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--ignore-certificate-errors')\n",
    "\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.get(\"https://www.citysearch.com/\")\n",
    "\n",
    "    #######################################################\n",
    "    #------extracting the links to individual cities------#\n",
    "    container = driver.find_element(\n",
    "        By.CSS_SELECTOR, \n",
    "        \"div.cities-container\"\n",
    "    )\n",
    "    cities = container.find_elements(\n",
    "        By.CSS_SELECTOR, \n",
    "        \"li:not([class*='state']) > a\"\n",
    "    )\n",
    "    city_links = [city.get_attribute(\"href\") for city in cities]\n",
    "\n",
    "    for state in states:\n",
    "        visited = set() # skipping business already scraped\n",
    "\n",
    "        pattern = re.compile(f\".*/{us_state_to_abbrev[state]}/.*\", re.IGNORECASE)\n",
    "        where_params = [\n",
    "            switch(param) \n",
    "            for param in [\n",
    "                link.replace(\"https://www.citysearch.com/\", \"\") \n",
    "                for link in city_links\n",
    "                if bool(pattern.match(link))\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "        for where_param in where_params:\n",
    "            business_list = []\n",
    "\n",
    "            for industry in [list(industry.keys())[0] for industry in industries]:\n",
    "                url = f\"https://www.citysearch.com/results?term={industry.strip().replace(' ', '%20')}&where={where_param}\"\n",
    "\n",
    "                print(\"--------------------state, params, industry--------------------\")\n",
    "                print(\"-------------------------\", url, \"----------------------------\")\n",
    "                print(state, where_param, industry)\n",
    "\n",
    "                driver.get(url)\n",
    "                job_cards_links = get_job_cards_links(driver)\n",
    "\n",
    "                if len(job_cards_links) == 0:\n",
    "                    continue\n",
    "\n",
    "                # visiting each job link for the current \\\n",
    "                # industry and scraping information\n",
    "                for job_cards_link in job_cards_links: \n",
    "                    if job_cards_link in visited:\n",
    "                        print('already visited skipping')\n",
    "                        continue\n",
    "\n",
    "                    visited.add(job_cards_link)\n",
    "\n",
    "                    print(\"------visiting profile: \", job_cards_link, \"------\")\n",
    "                    driver.get(job_cards_link)\n",
    "\n",
    "                    business_details_dict = business_details_to_dict(driver, \n",
    "                                                                     industry)\n",
    "\n",
    "\n",
    "                    try:\n",
    "                        print('looking for additional details')\n",
    "                        additional_info = WebDriverWait(driver, 5).until(\n",
    "                            EC.presence_of_element_located(\n",
    "                                (\n",
    "                                    By.CSS_SELECTOR, \n",
    "                                    'div.panel-container \\\n",
    "                                        > div.panel-details'\n",
    "                                )\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "                        business_details_dict['additional_info'] = \\\n",
    "                            additional_info.text\n",
    "\n",
    "                    except (NoSuchElementException, TimeoutException):\n",
    "                        print(\"No additional info container\")\n",
    "                        print(\"stopped at: \", business_details_dict)\n",
    "                        business_details_dict['additional_info'] = ''\n",
    "\n",
    "                    emails = set()\n",
    "\n",
    "                    external_link = business_details_dict['external-links-container']\n",
    "\n",
    "                    if external_link != '':\n",
    "                        await get_email(external_link, emails)\n",
    "                        await get_email_from_contact(driver, external_link, emails)\n",
    "                        print(\"emails have been updated these are emails\", emails)\n",
    "\n",
    "                    business_details_dict['emails'] = str(list(emails))\n",
    "\n",
    "                    print(business_details_dict)\n",
    "                    business_list.append(business_details_dict)\n",
    "                    # there's a lot of waiting in between,\\\n",
    "                    # don't think we need a long wait\n",
    "                    time.sleep(1)\n",
    "\n",
    "            save_to_csv(business_list, where_param)\n",
    "\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad89ba7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(main())\n",
    "loop.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6288fa8",
   "metadata": {},
   "source": [
    "## Data Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d15b67d",
   "metadata": {},
   "source": [
    "### **Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76106daf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import functools as ft\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1a2222",
   "metadata": {},
   "source": [
    "### **Keys**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5d05ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "state_dict = {\n",
    "                'California': 'ca', \n",
    "                'New Jersey': 'nj', \n",
    "                'New York': 'ny', \n",
    "                'Texas': 'tx'\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4a6fb1",
   "metadata": {},
   "source": [
    "### **Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd963855",
   "metadata": {},
   "source": [
    "Step 1) Using https://www.zipcodestogo.com gathering zip code of every city for the current state and converting it to a dataframe\n",
    "\n",
    "Step 2) During scraping, each city, state pair was exported to a separate .csv file. Here, we are combining all the results for each state into a dataframe, dropping rows that don't have a business name and phone number and dropping rows that don't have an address\n",
    "\n",
    "Step 3) From the dataframe created in step 2, extracting zip code from the values in the address column into a column call 'zip_code'\n",
    "\n",
    "Step 4) Merging dataframe from step 1 and step 3 by zip_code\n",
    "\n",
    "Step 5) Highlighting non-unique addresses and exporting the final result to a .xlxs file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88213105",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for state in state_dict.keys():\n",
    "    ##############################\n",
    "    #------getting zipcodes------#\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(f'https://www.zipcodestogo.com/{state}/')\n",
    "\n",
    "    table_rows = driver.find_elements(\n",
    "        By.CSS_SELECTOR, \n",
    "        'table.inner_table > tbody > tr'\n",
    "    )\n",
    "\n",
    "    zip_list = []\n",
    "    temp_key = {0: 'zip_code', 1: 'city', 2: 'county'}\n",
    "\n",
    "    for row in table_rows:\n",
    "        cols = row.find_elements(By.CSS_SELECTOR, 'td')\n",
    "        zip_dict = {'zip_code':'', 'city':'', 'county':''}\n",
    "\n",
    "        for idx, col in enumerate(cols[0:3]):\n",
    "            zip_dict[temp_key[idx]] = col.text\n",
    "\n",
    "        zip_list.append(zip_dict)\n",
    "\n",
    "    ny_zip_df = pd.DataFrame.from_dict(zip_list)\n",
    "    ny_zip_df = ny_zip_df.iloc[2:, :]\n",
    "    ny_zip_df = ny_zip_df.astype({'zip_code': str})\n",
    "    ny_zip_df.dtypes\n",
    "\n",
    "    driver.close()\n",
    "\n",
    "\n",
    "    #####################################\n",
    "    #------combining state results------#\n",
    "    # path = './results'\n",
    "    path = './' #path to where .csv file separated by city are\n",
    "    os.listdir(path)\n",
    "    csv_list = [\n",
    "        file \n",
    "        for file in os.listdir(path) \n",
    "        if file.endswith(\n",
    "            f\"_{state_dict[state]}.csv\"\n",
    "        )\n",
    "    ]\n",
    "    state_dfs = [\n",
    "        pd.read_csv(path + '/' + csv_name) \n",
    "        for csv_name in csv_list \n",
    "        if os.stat(\n",
    "            path + '/' + csv_name\n",
    "        ).st_size > 2\n",
    "    ]\n",
    "    state_df = pd.concat(state_dfs)\n",
    "    state_df.reset_index(drop=True ,inplace=True)\n",
    "    state_df = state_df[\n",
    "        [\n",
    "            'industry', \n",
    "            'business name', \n",
    "            'address', \n",
    "            'external link', \n",
    "            'phone number', \n",
    "            'additional_info', \n",
    "            'emails', \n",
    "            'business hours'\n",
    "        ]\n",
    "    ]\n",
    "    state_df.dropna(\n",
    "        subset=['business name', 'phone number'], \n",
    "        how='all', \n",
    "        inplace=True\n",
    "    )\n",
    "    state_df.dropna(subset=['address'], inplace=True)\n",
    "\n",
    "\n",
    "    #################################\n",
    "    #------extracting zip code------#\n",
    "    zip_code_pattern = re.compile(\"\\d{5}(-\\d{4})?$\")\n",
    "\n",
    "    state_df['zip_code'] = state_df.apply(\n",
    "        lambda row: str(re.search(zip_code_pattern, row['address']).group(0))\n",
    "        if not pd.isna(row['address'])\n",
    "        else row['address'], axis=1\n",
    "    )\n",
    "\n",
    "    # checking to see proper zip codes\n",
    "    state_df[~state_df['zip_code'].apply(\n",
    "        lambda x: str(x).startswith('9') and len(str(x)) == 5\n",
    "    )]\n",
    "\n",
    "    state_df = state_df.astype({'zip_code': str})\n",
    "    state_df.dtypes\n",
    "    # checking to see all zipcodes are accounted for\n",
    "    state_df['zip_code'].isna().sum()\n",
    "\n",
    "\n",
    "    #################################\n",
    "    #------merging by zip code------#\n",
    "    merge_list = [state_df, ny_zip_df]\n",
    "    df_final = ft.reduce(lambda left, right: pd.merge(left, right, on='zip_code'), \n",
    "                         merge_list)\n",
    "\n",
    "\n",
    "    ################################\n",
    "    #------finding duplicates------#\n",
    "    # finding duplicates\n",
    "    df_final['address'].value_counts()\n",
    "    rows = df_final.loc[df_final.duplicated(subset=['address'], keep=False)]\n",
    "    # index of duplicates\n",
    "    duplicates_mask = df_final['address'].duplicated(keep=False)\n",
    "    # Get indexes of non-unique values\n",
    "    non_unique_indexes = df_final[duplicates_mask].index.tolist()\n",
    "    repeats = df_final.iloc[non_unique_indexes]\n",
    "\n",
    "    def highlight_high_score(row):\n",
    "        return [\n",
    "                    'background-color: yellow' \n",
    "                    if row.name in non_unique_indexes \n",
    "                    else '' \n",
    "                    for _ in row\n",
    "                ]\n",
    "\n",
    "    # Highlighting non-unique addresses\n",
    "    df_final.sort_values(by='address', inplace=True)\n",
    "    styled_df = df_final.style.apply(highlight_high_score, axis=1)\n",
    "    styled_df.to_excel(\n",
    "        f'{state_dict[state]}_final.xlsx', \n",
    "        engine='openpyxl', \n",
    "        index=False\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
